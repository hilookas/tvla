#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/tvla/models/pi0/modular_pi0.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_pi0.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
from transformers.modeling_utils import PreTrainedModel

import torch
from torch import nn
from torch import Tensor
import torch.nn.functional as F  # noqa: N812
from tvla.models.gemma import GemmaForCausalLM, GemmaConfig
from tvla.models.paligemma import PaliGemmaForConditionalGeneration, PaliGemmaConfig
from tvla.models.gemma.modeling_gemma import (
    apply_rotary_pos_emb,
    eager_attention_forward,
)

import math
from .configuration_pi0 import Pi0Config


class Pi0PreTrainedModel(PreTrainedModel):
    config: Pi0Config


def create_sinusoidal_pos_embedding(
    time: torch.tensor,
    dimension: int,
    min_period: float,
    max_period: float,
    device="cpu",
) -> Tensor:
    """Computes sine-cosine positional embedding vectors for scalar positions."""
    if dimension % 2 != 0:
        raise ValueError(f"dimension ({dimension}) must be divisible by 2")

    if time.ndim != 1:
        raise ValueError("The time tensor is expected to be of shape `(batch_size, )`.")

    dtype = torch.float64
    fraction = torch.linspace(0.0, 1.0, dimension // 2, dtype=dtype, device=device)
    period = min_period * (max_period / min_period) ** fraction

    # Compute the outer product
    scaling_factor = 1.0 / period * 2 * math.pi
    sin_input = scaling_factor[None, :] * time[:, None]
    return torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)


def sample_beta(alpha, beta, bsize, device):
    alpha_t = torch.as_tensor(alpha, dtype=torch.float32, device=device)
    beta_t = torch.as_tensor(beta, dtype=torch.float32, device=device)
    dist = torch.distributions.Beta(alpha_t, beta_t)
    return dist.sample((bsize,))


def make_att_2d_masks(pad_masks, att_masks):
    """Copied from big_vision.

    Tokens can attend to valid inputs tokens which have a cumulative mask_ar
    smaller or equal to theirs. This way `mask_ar` int[B, N] can be used to
    setup several types of attention, for example:

      [[1 1 1 1 1 1]]: pure causal attention.

      [[0 0 0 1 1 1]]: prefix-lm attention. The first 3 tokens can attend between
          themselves and the last 3 tokens have a causal attention. The first
          entry could also be a 1 without changing behaviour.

      [[1 0 1 0 1 0 0 1 0 0]]: causal attention between 4 blocks. Tokens of a
          block can attend all previous blocks and all tokens on the same block.

    Args:
      input_mask: bool[B, N] true if its part of the input, false if padding.
      mask_ar: int32[B, N] mask that's 1 where previous tokens cannot depend on
        it and 0 where it shares the same attention mask as the previous token.
    """
    if att_masks.ndim != 2:
        raise ValueError(att_masks.ndim)
    if pad_masks.ndim != 2:
        raise ValueError(pad_masks.ndim)

    cumsum = torch.cumsum(att_masks, dim=1)
    att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]
    pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]
    return att_2d_masks & pad_2d_masks


def prepare_attention_masks_4d(att_2d_masks):
    """Helper method to prepare 4D attention masks for transformer."""
    att_2d_masks_4d = att_2d_masks[:, None, :, :]
    return torch.where(att_2d_masks_4d, 0.0, -2.3819763e38)


class Pi0Model(Pi0PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        paligemma_config = PaliGemmaConfig(use_norm=False)
        paligemma_config._vocab_size = 257152  # noqa: SLF001
        paligemma_config.image_token_index = 257152
        paligemma_config.text_config.hidden_size = 2048
        paligemma_config.text_config.intermediate_size = 16384
        paligemma_config.text_config.num_hidden_layers = 18
        paligemma_config.text_config.num_attention_heads = 8
        paligemma_config.text_config.head_dim = 256
        paligemma_config.text_config.num_key_value_heads = 1
        paligemma_config.text_config.hidden_activation = "gelu_pytorch_tanh"
        paligemma_config.text_config.torch_dtype = "float32"
        paligemma_config.text_config.vocab_size = 257152
        paligemma_config.text_config.adarms_cond_dim = None
        paligemma_config.vision_config.intermediate_size = 4304
        paligemma_config.vision_config.projection_dim = 2048
        paligemma_config.vision_config.projector_hidden_act = "gelu_fast"
        paligemma_config.vision_config.torch_dtype = "float32"

        gemma_expert_config = GemmaConfig(
            hidden_size=1024,
            intermediate_size=4096,
            num_hidden_layers=18,
            num_attention_heads=8,
            head_dim=256,
            num_key_value_heads=1,
            vocab_size=257152,
            hidden_activation="gelu_pytorch_tanh",
            torch_dtype="float32",
            adarms_cond_dim=1024 if self.config.pi05 else None,  # same as hidden_size
            use_norm=False,
        )

        self.paligemma = PaliGemmaForConditionalGeneration(config=paligemma_config)
        self.gemma_expert = GemmaForCausalLM(config=gemma_expert_config)

        self.gemma_expert.model.config._attn_implementation = "eager"  # noqa: SLF001
        self.paligemma.language_model.config._attn_implementation = "eager"

        self.action_in_proj = nn.Linear(32, gemma_expert_config.hidden_size)
        self.action_out_proj = nn.Linear(gemma_expert_config.hidden_size, 32)

        if self.config.pi05:
            self.time_mlp_in = nn.Linear(
                gemma_expert_config.hidden_size, gemma_expert_config.hidden_size
            )
            self.time_mlp_out = nn.Linear(
                gemma_expert_config.hidden_size, gemma_expert_config.hidden_size
            )
        else:
            self.state_proj = nn.Linear(32, gemma_expert_config.hidden_size)
            self.action_time_mlp_in = nn.Linear(
                2 * gemma_expert_config.hidden_size, gemma_expert_config.hidden_size
            )
            self.action_time_mlp_out = nn.Linear(
                gemma_expert_config.hidden_size, gemma_expert_config.hidden_size
            )

        torch.set_float32_matmul_precision("high")

        # self.sample_action = torch.compile(self.sample_action, mode="max-autotune")

    def sample_noise(self, shape, device):
        return torch.normal(
            mean=0.0,
            std=1.0,
            size=shape,
            dtype=torch.float32,
            device=device,
        )

    def sample_time(self, bsize, device):
        time_beta = sample_beta(1.5, 1.0, bsize, device)
        time = time_beta * 0.999 + 0.001
        return time.to(dtype=torch.float32, device=device)

    def embed_prefix(
        self, images, img_masks, lang_tokens, lang_masks
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Embed images with SigLIP and language tokens with embedding layer to prepare
        for PaliGemma transformer processing.
        """
        embs = []
        pad_masks = []
        att_masks = []

        # Process images
        for img, img_mask in zip(images, img_masks, strict=True):
            img_emb = self.paligemma.model.get_image_features(img)

            bsize, num_img_embs = img_emb.shape[:2]

            embs.append(img_emb)
            pad_masks.append(img_mask[:, None].expand(bsize, num_img_embs))

            # Create attention masks so that image tokens attend to each other
            att_masks += [0] * num_img_embs

        # Process language tokens
        lang_emb = self.paligemma.language_model.embed_tokens(lang_tokens)
        lang_emb_dim = lang_emb.shape[-1]
        lang_emb = lang_emb * math.sqrt(lang_emb_dim)

        embs.append(lang_emb)
        pad_masks.append(lang_masks)

        # full attention between image and language inputs
        num_lang_embs = lang_emb.shape[1]
        att_masks += [0] * num_lang_embs

        embs = torch.cat(embs, dim=1)
        pad_masks = torch.cat(pad_masks, dim=1)
        att_masks = torch.tensor(att_masks, dtype=torch.bool, device=pad_masks.device)

        # Get batch size from the first dimension of the concatenated tensors
        bsize = pad_masks.shape[0]
        att_masks = att_masks[None, :].expand(bsize, len(att_masks))

        return embs, pad_masks, att_masks

    def embed_suffix(self, state, noisy_actions, timestep):
        """Embed state, noisy_actions, timestep to prepare for Expert Gemma processing."""
        embs = []
        pad_masks = []
        att_masks = []

        if not self.config.pi05:
            if self.state_proj.weight.dtype == torch.float32:
                state = state.to(torch.float32)

            # Embed state
            state_emb = self.state_proj(state)

            embs.append(state_emb[:, None, :])
            bsize = state_emb.shape[0]
            device = state_emb.device

            state_mask = torch.ones(bsize, 1, dtype=torch.bool, device=device)
            pad_masks.append(state_mask)

            # Set attention masks so that image and language inputs do not attend to state or actions
            att_masks += [1]

        # Embed timestep using sine-cosine positional encoding with sensitivity in the range [0, 1]
        time_emb = create_sinusoidal_pos_embedding(
            timestep,
            self.action_in_proj.out_features,
            min_period=4e-3,
            max_period=4.0,
            device=timestep.device,
        )
        time_emb = time_emb.type(dtype=timestep.dtype)

        # Fuse timestep + action information using an MLP
        action_emb = self.action_in_proj(noisy_actions)

        if not self.config.pi05:
            time_emb = time_emb[:, None, :].expand_as(action_emb)
            action_time_emb = torch.cat([action_emb, time_emb], dim=2)

            # Apply MLP layers
            x = self.action_time_mlp_in(action_time_emb)
            x = F.silu(x)  # swish == silu
            action_time_emb = self.action_time_mlp_out(x)
            adarms_cond = None
        else:
            # time MLP (for adaRMS)
            x = self.time_mlp_in(time_emb)
            x = F.silu(x)  # swish == silu
            x = self.time_mlp_out(x)
            time_emb = F.silu(x)
            action_time_emb = action_emb
            adarms_cond = time_emb

        # Add to input tokens
        embs.append(action_time_emb)

        bsize, action_time_dim = action_time_emb.shape[:2]
        action_time_mask = torch.ones(
            bsize, action_time_dim, dtype=torch.bool, device=timestep.device
        )
        pad_masks.append(action_time_mask)

        # Set attention masks so that image, language and state inputs do not attend to action tokens
        att_masks += [1] + ([0] * (self.config.action_horizon - 1))

        embs = torch.cat(embs, dim=1)
        pad_masks = torch.cat(pad_masks, dim=1)
        att_masks = torch.tensor(att_masks, dtype=embs.dtype, device=embs.device)
        att_masks = att_masks[None, :].expand(bsize, len(att_masks))

        return embs, pad_masks, att_masks, adarms_cond

    def paligemma_with_expert_forward(
        self,
        inputs_embeds: list[torch.FloatTensor],
        attention_mask: torch.Tensor,
        position_ids: torch.LongTensor,
        adarms_cond: list[torch.Tensor] = [None, None],
    ):
        models = [self.paligemma.language_model, self.gemma_expert.model]

        # Process all layers
        for layer_idx in range(self.paligemma.config.text_config.num_hidden_layers):
            query_states = []
            key_states = []
            value_states = []
            gates = []
            for model_idx, hidden_states in enumerate(inputs_embeds):
                layer = models[model_idx].layers[layer_idx]
                hidden_states, gate = layer.input_layernorm(
                    hidden_states, cond=adarms_cond[model_idx]
                )  # noqa: PLW2901
                gates.append(gate)

                input_shape = hidden_states.shape[:-1]
                hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)
                query_state = (
                    layer.self_attn.q_proj(hidden_states)
                    .view(hidden_shape)
                    .transpose(1, 2)
                )
                key_state = (
                    layer.self_attn.k_proj(hidden_states)
                    .view(hidden_shape)
                    .transpose(1, 2)
                )
                value_state = (
                    layer.self_attn.v_proj(hidden_states)
                    .view(hidden_shape)
                    .transpose(1, 2)
                )

                query_states.append(query_state)
                key_states.append(key_state)
                value_states.append(value_state)

            # Concatenate and process attention
            query_states = torch.cat(query_states, dim=2)
            key_states = torch.cat(key_states, dim=2)
            value_states = torch.cat(value_states, dim=2)

            dummy_tensor = torch.zeros(
                query_states.shape[0],
                query_states.shape[2],
                query_states.shape[-1],
                device=query_states.device,
                dtype=query_states.dtype,
            )
            cos, sin = self.paligemma.model.language_model.rotary_emb(
                dummy_tensor, position_ids
            )
            query_states, key_states = apply_rotary_pos_emb(
                query_states, key_states, cos, sin, unsqueeze_dim=1
            )

            batch_size = query_states.shape[0]
            scaling = self.paligemma.language_model.layers[layer_idx].self_attn.scaling

            # Attention computation
            att_output, _ = eager_attention_forward(
                self.paligemma.language_model.layers[layer_idx].self_attn,
                query_states,
                key_states,
                value_states,
                attention_mask,
                scaling,
            )
            # Get head_dim from the current layer, not from the model
            head_dim = self.paligemma.language_model.layers[
                layer_idx
            ].self_attn.head_dim
            att_output = att_output.reshape(batch_size, -1, 1 * 8 * head_dim)

            # Process layer outputs
            outputs_embeds = []
            start_pos = 0
            for model_idx, hidden_states in enumerate(inputs_embeds):
                layer = models[model_idx].layers[layer_idx]
                end_pos = start_pos + hidden_states.shape[1]

                if att_output.dtype != layer.self_attn.o_proj.weight.dtype:
                    att_output = att_output.to(layer.self_attn.o_proj.weight.dtype)
                out_emb = layer.self_attn.o_proj(att_output[:, start_pos:end_pos])

                # first residual
                out_emb = hidden_states + out_emb * gates[model_idx]
                after_first_residual = out_emb.clone()
                out_emb, gate = layer.post_attention_layernorm(
                    out_emb, cond=adarms_cond[model_idx]
                )

                out_emb = layer.mlp(out_emb)
                # second residual
                out_emb = after_first_residual + out_emb * gate
                outputs_embeds.append(out_emb)
                start_pos = end_pos

            inputs_embeds = outputs_embeds

        # final norm
        outputs_embeds = []
        for model_idx, hidden_states in enumerate(inputs_embeds):
            out_emb, _ = models[model_idx].norm(
                hidden_states, cond=adarms_cond[model_idx]
            )
            outputs_embeds.append(out_emb)

        return [outputs_embeds[0], outputs_embeds[1]]

    def forward(
        self,
        images,
        img_masks,
        lang_tokens,
        lang_masks,
        state,
        actions,
        noise=None,
        time=None,
    ) -> Tensor:
        """Do a full training forward pass and compute the loss (batch_size x num_steps x num_motors)"""

        if noise is None:
            noise = self.sample_noise(actions.shape, actions.device)

        if time is None:
            time = self.sample_time(actions.shape[0], actions.device)

        time_expanded = time[:, None, None]
        x_t = time_expanded * noise + (1 - time_expanded) * actions
        u_t = noise - actions

        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(
            images, img_masks, lang_tokens, lang_masks
        )
        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = (
            self.embed_suffix(state, x_t, time)
        )

        pad_masks = torch.cat([prefix_pad_masks, suffix_pad_masks], dim=1)
        att_masks = torch.cat([prefix_att_masks, suffix_att_masks], dim=1)

        att_2d_masks = make_att_2d_masks(pad_masks, att_masks)
        position_ids = torch.cumsum(pad_masks, dim=1) - 1

        # Prepare attention masks
        att_2d_masks_4d = prepare_attention_masks_4d(att_2d_masks)

        (_, suffix_out) = self.paligemma_with_expert_forward(
            inputs_embeds=[prefix_embs, suffix_embs],
            attention_mask=att_2d_masks_4d,
            position_ids=position_ids,
            adarms_cond=[None, adarms_cond],
        )

        suffix_out = suffix_out[:, -self.config.action_horizon :].to(
            dtype=torch.float32
        )

        # Final action projection
        v_t = self.action_out_proj(suffix_out)

        return F.mse_loss(u_t, v_t, reduction="none")

    @torch.no_grad()
    def sample_action(
        self,
        images,
        img_masks,
        lang_tokens,
        lang_masks,
        state,
        noise=None,
        num_steps=10,
    ) -> Tensor:
        """Do a full inference forward and compute the action (batch_size x num_steps x num_motors)"""

        device = lang_tokens.device
        bsize = lang_tokens.shape[0]

        if noise is None:
            actions_shape = (bsize, self.config.action_horizon, self.config.action_dim)
            noise = self.sample_noise(actions_shape, device)

        prefix_embs, prefix_pad_masks, prefix_att_masks = self.embed_prefix(
            images, img_masks, lang_tokens, lang_masks
        )
        prefix_att_2d_masks = make_att_2d_masks(prefix_pad_masks, prefix_att_masks)
        prefix_position_ids = torch.cumsum(prefix_pad_masks, dim=1) - 1

        # Compute image and language key value cache
        prefix_att_2d_masks_4d = prepare_attention_masks_4d(prefix_att_2d_masks)
        assert self.paligemma.language_model.config._attn_implementation == "eager"  # noqa: SLF001

        prefix_output = self.paligemma.language_model.forward(
            inputs_embeds=prefix_embs,
            attention_mask=prefix_att_2d_masks_4d,
            position_ids=prefix_position_ids,
            past_key_values=None,
            use_cache=True,
            adarms_cond=None,
        )
        past_key_values = prefix_output.past_key_values

        dt = -1.0 / num_steps
        dt = torch.tensor(dt, dtype=torch.float32, device=device)

        x_t = noise
        time = torch.tensor(1.0, dtype=torch.float32, device=device)
        while time >= -dt / 2:
            expanded_time = time.expand(bsize)
            v_t = self.denoise_step(
                state,
                prefix_pad_masks,
                past_key_values,
                x_t,
                expanded_time,
            )

            # Euler step - use new tensor assignment instead of in-place operation
            x_t = x_t + dt * v_t
            time += dt
        return x_t

    def denoise_step(
        self,
        state,
        prefix_pad_masks,
        past_key_values,
        x_t,
        timestep,
    ):
        """Apply one denoising step of the noise `x_t` at a given timestep."""
        suffix_embs, suffix_pad_masks, suffix_att_masks, adarms_cond = (
            self.embed_suffix(state, x_t, timestep)
        )

        suffix_len = suffix_pad_masks.shape[1]
        batch_size = prefix_pad_masks.shape[0]
        prefix_len = prefix_pad_masks.shape[1]

        prefix_pad_2d_masks = prefix_pad_masks[:, None, :].expand(
            batch_size, suffix_len, prefix_len
        )

        suffix_att_2d_masks = make_att_2d_masks(suffix_pad_masks, suffix_att_masks)

        full_att_2d_masks = torch.cat([prefix_pad_2d_masks, suffix_att_2d_masks], dim=2)

        prefix_offsets = torch.sum(prefix_pad_masks, dim=-1)[:, None]
        position_ids = prefix_offsets + torch.cumsum(suffix_pad_masks, dim=1) - 1

        # Prepare attention masks
        full_att_2d_masks_4d = prepare_attention_masks_4d(full_att_2d_masks)
        assert self.gemma_expert.model.config._attn_implementation == "eager"  # noqa: SLF001

        suffix_output = self.gemma_expert.model.forward(
            inputs_embeds=suffix_embs,
            attention_mask=full_att_2d_masks_4d,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=False,
            adarms_cond=adarms_cond,
        )
        suffix_out = suffix_output.last_hidden_state[
            :, -self.config.action_horizon :
        ].to(dtype=torch.float32)

        return self.action_out_proj(suffix_out)


__all__ = ["Pi0Model"]
